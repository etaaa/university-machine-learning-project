{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gabriel Natter (e12321311)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 194.025 EML Project Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is a machine learning task focused on classifying Rocket League trickshots based on gameplay data. The goal is to predict the type of trickshot performed using features like player and ball movements, speeds, and control inputs, with classes split evenly in the dataset. I built a model to accurately identify these shots, processing the data to capture key patterns and testing different algorithms to find the best performer. The final submission is a set of predictions on a test dataset, formatted for evaluation. My approach aimed to balance simplicity and effectiveness, ensuring robust results for this university assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imported pandas for data handling, matplotlib for visualizations, and scikit-learn tools like DecisionTreeClassifier, RandomForestClassifier, GridSearchCV, and metrics for model training and evaluation. This gave me a compact toolkit to manage the classification task efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, confusion_matrix, make_scorer, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by loading the training and test datasets from CSV files since they were already prepared for the Rocket League trickshot classification task. My goal was to get a quick sense of the data's structure and quality before diving into preprocessing. I used pandas to read the files and checked the training data with info(), head(), and describe() to understand the columns, data types, and basic statistics. I also looked at the label distribution to confirm the classes were balanced. Checking for missing values and unique counts of categorical columns helped me spot any potential issues early, like nulls or high-cardinality features that might complicate modeling. I ran a correlation check on numeric columns to see if any features were strongly related, which could guide feature selection later. This initial step was about building a solid foundation for the rest of the project.\n",
    "\n",
    "A Dataset description can be found [here](https://www.kaggle.com/competitions/194-025-eml-project-competition/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/rocketskillshots_train.csv\")\n",
    "test = pd.read_csv(\"data/rocketskillshots_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.info())\n",
    "display(train.head())\n",
    "display(train.describe())\n",
    "display(train[\"label\"].value_counts(normalize=True))\n",
    "display(train.isnull().sum())\n",
    "display(train.select_dtypes(include=\"object\").nunique())\n",
    "train.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began preprocessing by defining a list of summary features, since these capture important statistical patterns in the trickshot data. I separated the training and test data into timestamped and summary portions using the \"window_id\" column. The timestamped data has the play-by-play details, while the summary data holds precomputed stats I wanted to keep. Splitting them this way let me process each part appropriately while preserving all the useful info for later merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_features = [\n",
    "    \"BallAcceleration_skew\",\n",
    "    \"Time_skew\",\n",
    "    \"DistanceWall_skew\", \n",
    "    \"DistanceCeil_skew\",\n",
    "    \"DistanceBall_skew\",\n",
    "    \"PlayerSpeed_skew\", \n",
    "    \"BallSpeed_skew\",\n",
    "    #\"up_skew\",\n",
    "    \"accelerate_skew\",\n",
    "    \"slow_skew\", \n",
    "    \"goal_skew\",\n",
    "    #\"left_skew\",\n",
    "    \"boost_skew\",\n",
    "    #\"camera_skew\", \n",
    "    #\"down_skew\",\n",
    "    #\"right_skew\",\n",
    "    \"slide_skew\",\n",
    "    \"jump_skew\"\n",
    "]\n",
    "\n",
    "train_timestamp = train[train[\"window_id\"].notna()].copy()\n",
    "test_timestamp = test[test[\"window_id\"].notna()].copy()\n",
    "\n",
    "train_summary = train[train[\"window_id\"].isna()][[\"id\"] + summary_features].copy()\n",
    "test_summary = test[test[\"window_id\"].isna()][[\"id\"] + summary_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a function to define aggregation rules for the timestamped data, grouping by \"id\" to summarize each trickshot. I picked rules like taking the last value for distances and means or extremes for speeds and actions because they reflect the key moments and trends in a shot without bloating the dataset. I applied these rules to both training and test sets, flattened the column names for clarity, and merged the summary features back in to enrich the data with skew stats. This approach condensed the data into a format that's both compact and informative for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregation_rules(df):\n",
    "    aggregation_rules = {\n",
    "        \"BallAcceleration\": [\"last\"],\n",
    "        \"Time\": [\"max\"],\n",
    "        \"DistanceWall\": [\"mean\", \"min\", \"max\"],\n",
    "        \"DistanceCeil\": [\"mean\", \"min\", \"max\"],\n",
    "        \"DistanceBall\": [\"mean\", \"min\", \"max\"],\n",
    "        \"PlayerSpeed\": [\"mean\", \"min\", \"max\", \"last\"],\n",
    "        \"BallSpeed\": [\"mean\", \"min\", \"max\", \"last\"],\n",
    "        #\"up\": [\"mean\", \"max\"],\n",
    "        \"accelerate\": [\"mean\", \"max\"],\n",
    "        \"slow\": [\"mean\", \"max\"],\n",
    "        \"goal\": [\"mean\", \"max\"],\n",
    "        #\"left\": [\"mean\", \"max\"],\n",
    "        \"boost\": [\"mean\", \"max\"],\n",
    "        #\"camera\": [\"mean\", \"max\"],\n",
    "        #\"down\": [\"mean\", \"max\"],\n",
    "        #\"right\": [\"mean\", \"max\"],\n",
    "        \"slide\": [\"mean\", \"max\"],\n",
    "        \"jump\": [\"mean\", \"max\"]\n",
    "    }\n",
    "\n",
    "    if \"label\" in df.columns:\n",
    "        aggregation_rules[\"label\"] = [\"first\"]\n",
    "        \n",
    "    return aggregation_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_timestamp.groupby(\"id\").agg(get_aggregation_rules(train)).reset_index()\n",
    "test = test_timestamp.groupby(\"id\").agg(get_aggregation_rules(test)).reset_index()\n",
    "\n",
    "train.columns = [\n",
    "    f\"{col[0]}_{col[1]}\" if (col[1] != \"\" and col[0] != \"label\") else col[0]\n",
    "    for col in train.columns\n",
    "]\n",
    "test.columns = [\n",
    "    f\"{col[0]}_{col[1]}\" if col[1] != \"\" else col[0] \n",
    "    for col in test.columns\n",
    "]\n",
    "\n",
    "train = train.merge(train_summary, on=\"id\", how=\"left\")\n",
    "test = test.merge(test_summary, on=\"id\", how=\"left\")\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aggregation, I split the training data into features and labels, dropping \"id\" since it's just an identifier. I used train_test_split with a 20% validation set, setting a random seed for reproducibility and stratifying to maintain the even class balance. This gave me clean training and validation sets ready for model training, ensuring the data was well-structured and representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[\"label\"]\n",
    "X = train.drop(columns=[\"id\", \"label\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to test a Decision Tree and a Random Forest for the trickshot classification task since they’re straightforward and handle multiclass problems well, especially with balanced classes. I set a random seed for consistency across runs. For evaluation, I used accuracy and macro F1 score because they give a clear picture of performance on a balanced dataset, with F1 focusing on per-class quality. I wrote a function to run 5-fold cross-validation on the training set to get a robust sense of how each model performs, then trained and tested on the validation set to see real results. I included classification reports and confusion matrices to dig into per-class performance and spot any misclassifications visually. This setup let me compare the models fairly and understand their strengths before picking one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    \"Accuracy\": make_scorer(accuracy_score),\n",
    "    \"F1\": make_scorer(f1_score, average=\"macro\") # Good for balanced multiclass\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model):\n",
    "    print(model_name, \"\\n\")\n",
    "    \n",
    "    for score_name, scorer in scorers.items():\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scorer)\n",
    "        print(f\"{score_name}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    evaluate_model(model_name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to tune the Random Forest since it showed promise during initial model selection. I set up a grid search over key parameters like the number of trees, max depth, and sample splits to find the best combination for performance. I used 5-fold cross-validation and focused on macro F1 score to optimize for balanced class performance, as the dataset has evenly split classes. Running it with all available cores saved time. After finding the best parameters, I evaluated the tuned model using the same metrics and visuals as before to confirm it improved on the default setup. This step was about squeezing out better accuracy and reliability from the Random Forest without overcomplicating the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [6, 8, 10],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "evaluate_model(\"Tuned Random Forest\", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained the final Random Forest model on the full training data to make the most of all available information, as this usually boosts performance for predictions. I then used it to predict labels for the test set, keeping only the \"id\" column and predictions to match the submission format. I renamed the columns to \"ID\" and \"LABEL\" as required and saved the results to a CSV file. This step was about wrapping up the project cleanly, ensuring the output was ready for evaluation without any extra fuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "test[\"pred\"] = model.predict(test.drop(columns=[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[[\"id\", \"pred\"]].rename(columns={\"id\": \"ID\", \"pred\": \"LABEL\"})\n",
    "test.to_csv(\"submission/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I picked a Random Forest for the final submission because it outperformed the Decision Tree in initial tests, showing better accuracy and F1 scores across balanced classes. I tuned it with grid search, testing different tree counts, depths, and splits, settling on the best combo for macro F1 to ensure solid per-class performance. For data processing, I split the timestamped and summary data to handle them separately, aggregating the timestamped part with rules like last values for distances and means for actions to capture trickshot patterns concisely. I merged in skew features to add context, then split the data with stratification to keep the class balance. This approach kept the dataset manageable and focused on key shot dynamics, which helped the model learn effectively without unnecessary complexity. Working on this project was a great experience - I really enjoyed the challenge and had fun putting it all together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
