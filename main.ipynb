{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gabriel Natter (e12321311)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 194.025 EML Project Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I worked on a classification task as part of a university machine learning competition. The dataset and competition were hosted on Kaggle, where the goal was to develop machine learning models that perform well on an unseen test set. Submissions are scored on a leaderboard, and higher scores translate into more points for the course.\n",
    "\n",
    "The main objective was to beat several baseline models provided by the course organizers, such as random guessing and majority voting, and ideally perform better than progressively stronger models for maximum points. To achieve this, I experimented with different algorithms, preprocessing strategies, and evaluation methods.\n",
    "\n",
    "I started by exploring and understanding the dataset in detail, including data types, distributions, missing values, and class imbalance. Based on these insights, I implemented appropriate preprocessing steps to clean and prepare the data.\n",
    "\n",
    "The core of the project involves training and evaluating at least two different models. I chose to focus on Decision Trees and Random Forests, both of which are well-suited for classification tasks and provide a good balance between interpretability and performance. I used train-validation splits and cross-validation to assess model performance more reliably and avoid overfitting. To further improve the models, I performed hyperparameter tuning using grid search.\n",
    "\n",
    "Throughout the notebook, I evaluated models using various metrics including accuracy and confusion matrices to better understand not just how often the models were right, but how they were getting things right or wrong.\n",
    "\n",
    "The final part of the project involves selecting one model for submission. This decision was based on both performance metrics and the model's generalization behavior on validation data. All relevant steps — including preprocessing choices, modeling decisions, and performance evaluations — are documented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Library Imports and Setup](#1-library-imports-and-setup)\n",
    "2. [Data Exploration](#2-data-exploration)\n",
    "3. [Data Preprocessing](#3-data-preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I import all the necessary Python libraries used throughout the notebook. These include tools for data manipulation, model training and evaluation, as well as hyperparameter tuning and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training any models, it's important to understand the structure and characteristics of the data. Below, I load the training and test datasets provided in the competition. I also perform some basic exploration to get an initial feel for the data, such as looking at the first few rows, checking data types and null values, and reviewing the label distribution.\n",
    "\n",
    "Dataset description can be found [here](https://www.kaggle.com/competitions/194-025-eml-project-competition/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/rocketskillshots_train.csv\")\n",
    "test = pd.read_csv(\"data/rocketskillshots_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.info())\n",
    "display(train.head())\n",
    "display(train.describe())\n",
    "display(train[\"label\"].value_counts(normalize=True))\n",
    "train.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_aggregation_rules(df):\n",
    "    aggregation_rules = {\n",
    "        \"BallAcceleration\": [\"mean\", \"max\", \"min\"],\n",
    "        \"Time\": \"sum\",\n",
    "        \"DistanceWall\": [\"mean\", \"max\", \"min\"],\n",
    "        \"DistanceCeil\": [\"mean\", \"max\", \"min\"],\n",
    "        \"DistanceBall\": [\"mean\", \"max\", \"min\"],\n",
    "        \"PlayerSpeed\": [\"mean\", \"max\", \"min\"],\n",
    "        \"BallSpeed\": [\"mean\", \"max\", \"min\"],\n",
    "        \"up\": [\"mean\", \"max\", \"min\"],\n",
    "        \"accelerate\": [\"mean\", \"max\", \"min\"],\n",
    "        \"slow\": [\"mean\", \"max\", \"min\"],\n",
    "        \"goal\": [\"mean\", \"max\", \"min\"],\n",
    "        \"left\": [\"mean\", \"max\", \"min\"],\n",
    "        \"boost\": [\"mean\", \"max\", \"min\"],\n",
    "        \"camera\": [\"mean\", \"max\", \"min\"],\n",
    "        \"down\": [\"mean\", \"max\", \"min\"],\n",
    "        \"right\": [\"mean\", \"max\", \"min\"],\n",
    "        \"slide\": [\"mean\", \"max\", \"min\"],\n",
    "        \"jump\": [\"mean\", \"max\", \"min\"],\n",
    "        \"BallAcceleration_skew\": \"first\",\n",
    "        \"Time_skew\": \"first\",\n",
    "        \"DistanceWall_skew\": \"first\",\n",
    "        \"DistanceCeil_skew\": \"first\",\n",
    "        \"DistanceBall_skew\": \"first\",\n",
    "        \"PlayerSpeed_skew\": \"first\",\n",
    "        \"BallSpeed_skew\": \"first\",\n",
    "        \"up_skew\": \"first\",\n",
    "        \"accelerate_skew\": \"first\",\n",
    "        \"slow_skew\": \"first\",\n",
    "        \"goal_skew\": \"first\",\n",
    "        \"left_skew\": \"first\",\n",
    "        \"boost_skew\": \"first\",\n",
    "        \"camera_skew\": \"first\",\n",
    "        \"down_skew\": \"first\",\n",
    "        \"right_skew\": \"first\",\n",
    "        \"slide_skew\": \"first\",\n",
    "        \"jump_skew\": \"first\"\n",
    "    }\n",
    "    if \"label\" in df.columns:\n",
    "        aggregation_rules[\"label\"] = \"first\"\n",
    "    return aggregation_rules\n",
    "\n",
    "train = train.groupby(\"id\").agg(get_aggregation_rules(train)).reset_index()\n",
    "test = test.groupby(\"id\").agg(get_aggregation_rules(test)).reset_index()\n",
    "\n",
    "train.columns = [\n",
    "    f\"{col[0]}_{col[1]}\" if (col[1] != \"\" and col[0] != \"label\") else col[0]\n",
    "    for col in train.columns\n",
    "]\n",
    "test.columns = [\n",
    "    f\"{col[0]}_{col[1]}\" if col[1] != \"\" else col[0] \n",
    "    for col in test.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping unnecessary columns, I separated the label column (`label`) from the feature set. `X` contains all input features, and `y` contains the target variable. To evaluate model performance before making final predictions, I then split the data into training and validation sets using an 80/20 ratio. A fixed random seed ensures that results are reproducible across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[\"label\"]\n",
    "X = train.drop(columns=[\"id\", \"label\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I compare the performance of two classification models: a Decision Tree and a Random Forest, both with default parameters. I evaluate each model using accuracy, precision, recall, and F1 score on the validation set, and also compute 5-fold cross-validation accuracy to get a more robust estimate of each model’s generalization ability.\n",
    "\n",
    "This comparison helps identify which model is worth tuning further. Random Forest typically performs better due to ensemble averaging, but I include both for completeness.\n",
    "\n",
    "Each model is passed through two evaluation functions:\n",
    "- `evaluate_classification`: measures performance on the held-out validation set\n",
    "- `evaluate_with_cv`: computes average accuracy across 5 folds of cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(name, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds, average=\"weighted\")\n",
    "    recall = recall_score(y_test, preds, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False).head(10))\n",
    "\n",
    "    print(f\"{name} Evaluation:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_cv(name, model):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    print(f\"{name} (5-fold CV Accuracy): {scores.mean():.4f} ± {scores.std():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    evaluate_classification(name, model)\n",
    "    evaluate_with_cv(name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating the baseline models, I decided to continue with the Random Forest classifier for hyperparameter tuning. Although the Decision Tree achieved slightly better performance on the validation set and in cross-validation, Random Forests are generally more robust and less prone to overfitting due to their ensemble nature.\n",
    "\n",
    "To optimize its performance, I used `GridSearchCV` to tune two important hyperparameters:\n",
    "\n",
    "- `n_estimators`: the number of trees in the forest  \n",
    "- `max_depth`: the maximum depth of each tree\n",
    "\n",
    "A 5-fold cross-validation was used during the grid search to evaluate each parameter combination. This ensures the model is optimized for generalization and not just a single train-validation split.\n",
    "\n",
    "After training, I retrieved and evaluated the best model found during the search (`best_estimator_`) using the same classification metrics as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300],\n",
    "    \"max_depth\": [6, 8, 10, 15, 20, 25],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score:  \", grid_search.best_score_)\n",
    "\n",
    "model = grid_search.best_estimator_\n",
    "evaluate_classification(\"Tuned Random Forest\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting and training the final model, I used it to predict the labels on the test dataset. These predictions are made on a row-level basis, meaning each individual row in the test set receives a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "test[\"pred\"] = model.predict(test.drop(columns=[\"id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[[\"id\", \"pred\"]].rename(columns={\"id\": \"ID\", \"pred\": \"LABEL\"})\n",
    "test.to_csv(\"submission/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I experimented with both a Decision Tree and a Random Forest classifier. Although the Decision Tree showed slightly better validation performance, I chose to proceed with the Random Forest due to its better generalization capabilities and robustness to overfitting. To optimize its performance, I used `GridSearchCV` with 5-fold cross-validation to tune key hyperparameters (`n_estimators` and `max_depth`).\n",
    "\n",
    "In terms of data preprocessing, I removed columns with more than 50% missing values to simplify the pipeline and avoid the complexity of imputation. I also dropped the `window_id` column, as it acted as a group identifier and had no predictive value. Finally, I chose to use a majority voting strategy across rows sharing the same `id` to generate the final predictions, as this aggregation method improved performance for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Finish table of contents\n",
    "- Fix imports\n",
    "- Eventuell die ..._skew column noch miteinbeziehen\n",
    "- Tune feature aggregating\n",
    "- Rewrite most comments, as i removed majority voting approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
